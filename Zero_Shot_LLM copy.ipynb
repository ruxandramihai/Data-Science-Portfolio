{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs6bwCEKdRWh",
    "outputId": "bbc9060c-d401-40a6-c4c1-946f1e1845d1"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git # installing latest version of transformers library\n",
    "!pip install torch # torch\n",
    "!pip install peft # necessary for finetuning of the large model via LoRA approach\n",
    "!pip install bitsandbytes # necessary for quantiziation\n",
    "!pip install evaluate # extension of the transformers library\n",
    "!pip install datasets # extension of the transformers library\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UUGmVuHelEa"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding)\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from peft import get_peft_model\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
    "import logging\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuZTgXKPeizR"
   },
   "outputs": [],
   "source": [
    "def load_dataset(clean=True, mount_drive=True):\n",
    "    \"\"\"\n",
    "    Load datasets from JSON files in Google Drive and optionally clean NaN values\n",
    "\n",
    "    Args:\n",
    "        clean (bool): If True, remove rows with NaN values in the text column\n",
    "        mount_drive (bool): If True, mount Google Drive (only needs to be done once per session)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing train, val, and test dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    if mount_drive:\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "    dataset_folder = '/content/drive/MyDrive/dataset_mental_health'\n",
    "\n",
    "    train_path = os.path.join(dataset_folder, 'train_data.json')\n",
    "    val_path = os.path.join(dataset_folder, 'val_data.json')\n",
    "    test_path = os.path.join(dataset_folder, 'test_data.json')\n",
    "\n",
    "\n",
    "    with open(train_path, \"r\") as f:\n",
    "        train = json.load(f)\n",
    "    with open(val_path, \"r\") as f:\n",
    "        val = json.load(f)\n",
    "    with open(test_path, \"r\") as f:\n",
    "        test = json.load(f)\n",
    "\n",
    "    train_df = pd.DataFrame(train[\"data\"])\n",
    "    val_df = pd.DataFrame(val[\"data\"])\n",
    "    test_df = pd.DataFrame(test[\"data\"])\n",
    "\n",
    "    if clean:\n",
    "        train_len_before = len(train_df)\n",
    "        val_len_before = len(val_df)\n",
    "        test_len_before = len(test_df)\n",
    "\n",
    "        # drop NaN values\n",
    "        train_df = train_df.dropna(subset=['text'])\n",
    "        val_df = val_df.dropna(subset=['text'])\n",
    "        test_df = test_df.dropna(subset=['text'])\n",
    "\n",
    "        train_removed = train_len_before - len(train_df)\n",
    "        val_removed = val_len_before - len(val_df)\n",
    "        test_removed = test_len_before - len(test_df)\n",
    "\n",
    "        print(f\"Removed {train_removed} rows with NaN values from training dataset\")\n",
    "        print(f\"Removed {val_removed} rows with NaN values from validation dataset\")\n",
    "        print(f\"Removed {test_removed} rows with NaN values from testing dataset\")\n",
    "        print(f\"Total removed: {train_removed + val_removed + test_removed} rows\")\n",
    "\n",
    "    return {'train': train_df, 'val': val_df, 'test': test_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czkpy1YFer6u",
    "outputId": "5c054ef7-49cd-45c6-8464-5385576e348f"
   },
   "outputs": [],
   "source": [
    "dataset= load_dataset(clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svX-cXjleukT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqXCJSsGf7K1"
   },
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    'neutral': 0,\n",
    "    'moderate': 1,\n",
    "    'high-risk': 2\n",
    "}\n",
    "\n",
    "EMOTION_INDEX = {\n",
    "    0: 'neutral',\n",
    "    1: 'moderate',\n",
    "    2: 'high-risk'\n",
    "}\n",
    "\n",
    "CLASSES = 3\n",
    "DATACOLUMN = 'text'\n",
    "LABELCOLUMN = 'my_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRUjwGmEiqU8"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm8JWzh4lyNG"
   },
   "source": [
    "Zero-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cun4d1NroePn"
   },
   "source": [
    "Text generation piepline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyLCirLLzpBk"
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enables 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # double quantization for potentially higher accuracy (optional)\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type (specifics depend on hardware and library)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Compute dtype for improved efficiency (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=0)\n",
    "pipeline(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2VVCrn21P3q"
   },
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EltuLIyi1Yaw"
   },
   "outputs": [],
   "source": [
    "def classify_text(text, tokenizer, model, device=\"cuda\"):\n",
    "    prompt = f\"\"\"You are a mental health assistant AI.\n",
    "Given a user's text, classify the emotional distress level as one of the following three categories: neutral, moderate, or high risk.\n",
    "Respond with only one of these three labels.\n",
    "\n",
    "Text: {text}\n",
    "Label:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    if \"Label:\" in generated:\n",
    "        return generated.split(\"Label:\")[-1].strip().split()[0].lower()\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySoPb0_D2OoE"
   },
   "outputs": [],
   "source": [
    "test_df = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj2LtFPw2f2D",
    "outputId": "1a380f8c-0f15-4e87-d7d3-f15dea075b63"
   },
   "outputs": [],
   "source": [
    "test_df[\"gemma2_predicted_label\"] = test_df[\"text\"].apply(\n",
    "    lambda x: classify_text(x, tokenizer, model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXkqKzkn3VDi"
   },
   "source": [
    "Using pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRriE9VS2jlg",
    "outputId": "1774cb92-762a-4581-a19d-eb3d6046cffc"
   },
   "outputs": [],
   "source": [
    "class RiskLabel(BaseModel):\n",
    "    label: Literal[\"neutral\", \"moderate\", \"high risk\"] = Field(\n",
    "        description=\"The predicted mental health risk level for the given message.\"\n",
    "    )\n",
    "\n",
    "schema = RiskLabel.schema_json(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tVqSkTE32VC"
   },
   "outputs": [],
   "source": [
    "def classify_text_structured(text, tokenizer, model, device=\"cuda\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant that classifies mental health risk in user messages.\n",
    "\n",
    "Output a JSON object matching the following schema:\n",
    "{schema}\n",
    "\n",
    "Classify this message:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "Respond only with the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        # Extract JSON string from the output\n",
    "        json_str = generated[generated.index(\"{\") : generated.rindex(\"}\") + 1]\n",
    "        parsed = json.loads(json_str)\n",
    "        return parsed.get(\"label\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOnBucRkP8pV"
   },
   "outputs": [],
   "source": [
    "test_df[\"pydantic_gemma2_predicted_label\"] = test_df[\"text\"].apply(\n",
    "    lambda x: classify_text_structured(x, tokenizer, model, device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbdzVPb25RRv"
   },
   "outputs": [],
   "source": [
    "test_df['gemma2_predicted_label'] = test_df['gemma2_predicted_label'].replace({\n",
    "    'high': 'high-risk'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4vVKHAK5i3M"
   },
   "outputs": [],
   "source": [
    "test_df['gemma2_predicted_label_id'] = test_df['gemma2_predicted_label'].map(LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83uKLpWV5tnh",
    "outputId": "c1d9d5d3-8677-4cc9-c00d-b03839bb4150"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# quick sanity-check for NaNs\n",
    "print(\"NaNs in predicted IDs:\", test_df['gemma2_predicted_label_id'].isna().sum())\n",
    "print(\"NaNs in true IDs:     \", test_df['true_label_id'].isna().sum())\n",
    "\n",
    "# drop any row that still has a NaN (should be zero now)\n",
    "mask = ~(test_df['gemma2_predicted_label_id'].isna() | test_df['true_label_id'].isna())\n",
    "y_pred = test_df.loc[mask, 'gemma2_predicted_label_id'].astype(int)\n",
    "y_true = test_df.loc[mask, 'true_label_id'].astype(int)\n",
    "\n",
    "# metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Macro-averaged F1: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=['neutral (0)', 'moderate (1)', 'high-risk (2)']\n",
    "))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWGPDgeg7IKG",
    "outputId": "4ab31f7d-32d1-489c-cc77-423c0f781411"
   },
   "outputs": [],
   "source": [
    "test_df['gemma2_predicted_label_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Obsh3p67O9q"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('/content/drive/MyDrive/results/gemma2_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR7ZxS2iQPv5"
   },
   "source": [
    "Gemma 3 (12B parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-12b-pt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "access_token = \" \"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=access_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wqHjNBMdgO5"
   },
   "outputs": [],
   "source": [
    "def classify_text(text, tokenizer, model, device=\"cuda\"):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a mental health assistant AI.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Given the following text, classify the emotional distress level as one of the following three categories: neutral, moderate, or high risk. Respond with only one of these three labels.\\nText: {text}\"}\n",
    "    ]\n",
    "\n",
    "    # template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    #  response\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    lower_generated = generated.lower().strip()\n",
    "    if \"neutral\" in lower_generated:\n",
    "        return \"neutral\"\n",
    "    elif \"moderate\" in lower_generated:\n",
    "        return \"moderate\"\n",
    "    elif \"high\" in lower_generated or \"high risk\" in lower_generated or \"high-risk\" in lower_generated:\n",
    "        return \"high-risk\"\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVDCXoaHdnZ2"
   },
   "outputs": [],
   "source": [
    "test_df = dataset['test']\n",
    "test_df[\"gemma3_predicted_label\"] = test_df[\"text\"].apply(\n",
    "    lambda x: classify_text(x, tokenizer, model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgT_tW6Kj0-Z"
   },
   "outputs": [],
   "source": [
    "test_df['gemma3_predicted_label_id'] = test_df['gemma3_predicted_label'].map(LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksfZPXMgkN0s"
   },
   "outputs": [],
   "source": [
    "test_df['true_label_id'] = test_df['my_label'].map(LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGdAeE53jZ-I",
    "outputId": "337ec830-c75f-4ab7-f54e-98e92a14d586"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"NaNs in predicted IDs:\", test_df['gemma3_predicted_label_id'].isna().sum())\n",
    "print(\"NaNs in true IDs:     \", test_df['true_label_id'].isna().sum())\n",
    "\n",
    "mask = ~(test_df['gemma3_predicted_label_id'].isna() | test_df['true_label_id'].isna())\n",
    "y_pred = test_df.loc[mask, 'gemma3_predicted_label_id'].astype(int)\n",
    "y_true = test_df.loc[mask, 'true_label_id'].astype(int)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Macro-averaged F1: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=['neutral (0)', 'moderate (1)', 'high-risk (2)']\n",
    "))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkRgBLBkj5Ca"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('/content/drive/MyDrive/results/gemma3_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
